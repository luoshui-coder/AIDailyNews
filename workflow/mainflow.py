import os, json, datetime, glob
from workflow.gpt.summary import evaluate_article_with_gpt
import workflow.article.rss as rss
import workflow.article.blog as blog
import time
from loguru import logger

def execute(rss_resource="workflow/resources"):
    """
    ä¸»æ‰§è¡Œå‡½æ•°ï¼Œå¤„ç†RSSæ–‡ç« å¹¶ç”Ÿæˆæ—¥æŠ¥
    
    Args:
        rss_resource: RSSé…ç½®æ–‡ä»¶æ‰€åœ¨ç›®å½•è·¯å¾„
    
    æµç¨‹:
    1. æ£€æŸ¥ç¼“å­˜
    2. è·å–RSSæ–‡ç« 
    3. ä¿å­˜æ–‡ç« (å¦‚æœå¯ç”¨ç¼“å­˜)
    4. ç­›é€‰ä¼˜è´¨æ–‡ç« 
    5. ç”Ÿæˆmarkdownæ ¼å¼çš„æ—¥æŠ¥
    """
    # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„ç¼“å­˜æ–‡ä»¶
    cache_folder, cache_file = find_valid_file()
    # è§£æRSSæ–‡ç« (å¦‚æœæœ‰ç¼“å­˜åˆ™ä»ç¼“å­˜è¯»å–)
    origin_article_list = parse_daily_rss_article(rss_resource, cache_file)
    # å¦‚æœå¯ç”¨ç¼“å­˜ï¼Œä¿å­˜è§£æç»“æœ
    if cache_folder:
        save_article(origin_article_list, cache_folder)
    # ç­›é€‰è¯„åˆ†é«˜çš„æ–‡ç« 
    articles = find_favorite_article(origin_article_list)
    # ç”Ÿæˆmarkdownæ ¼å¼çš„æ—¥æŠ¥
    blog.make_daily_markdown_with(articles, origin_article_list)

def parse_daily_rss_article(rss_resource, cache_file=None):
    """
    è·å–RSSæ–‡ç« ä¿¡æ¯
    
    Args:
        rss_resource: RSSé…ç½®æ–‡ä»¶ç›®å½•
        cache_file: ç¼“å­˜æ–‡ä»¶è·¯å¾„(å¦‚æœæœ‰)
    
    Returns:
        list: RSSæ–‡ç« åˆ—è¡¨
    """
    # å¦‚æœæœ‰ç¼“å­˜æ–‡ä»¶ï¼Œç›´æ¥ä»ç¼“å­˜è¯»å–
    if cache_file:
        return decode_article(cache_file)
    
    # åŠ è½½RSSé…ç½®
    rss_items = rss.load_rss_configs(rss_resource)

    # è§£ææ‰€æœ‰RSSæº
    daily_rss = []
    for item in rss_items:
        rss_list = rss.parse_rss_config(item)
        for rss_item in rss_list:
            daily_rss.append(rss_item)
            logger.info(f"date: {rss_item.date}, link: {rss_item.link}")
    return daily_rss

def find_favorite_article(rss_articles):
    """
    ç­›é€‰è¯„åˆ†æœ€é«˜çš„æ–‡ç« 
    
    ç­›é€‰æ ‡å‡†:
    - è¯„åˆ† >= 7åˆ†çš„æ–‡ç« æ‰ä¼šè¢«è€ƒè™‘
    - è¯„åˆ† = 10åˆ†çš„æ–‡ç« ä¼˜å…ˆé€‰æ‹©
    - æ¯ä¸ªRSSæºæœ‰ç‹¬ç«‹çš„è¾“å‡ºæ•°é‡é™åˆ¶
    - æœ€ç»ˆè¾“å‡ºä¸è¶…è¿‡è®¾å®šçš„æœ€å¤§æ–‡ç« æ•°
    - å»é™¤é‡å¤æˆ–ç›¸ä¼¼çš„æ–‡ç« 
    """
    # é™åˆ¶åˆ†ææ–‡ç« æ•°é‡
    max_analyze_nums = 100
    rss_articles = rss_articles[:max_analyze_nums]
    max_article_nums = int(os.environ.get("MAX_ARTICLE_NUMS", "30"))
    
    # æŒ‰RSSæºåˆ†ç»„
    rss_resource = {}
    for article in rss_articles:
        if not article.summary:
            continue
        rss_category = article.info["title"]
        if rss_category in rss_resource.keys():
            rss_resource[rss_category].append(article)
        else:
            rss_resource[rss_category] = [article]

    # å¤„ç†æ¯ä¸ªRSSæºçš„æ–‡ç« 
    show_articles = []
    seen_titles = {}  # ç”¨äºè®°å½•å·²è§è¿‡çš„æ ‡é¢˜
    
    for key, articles in rss_resource.items():
        time.sleep(2)
        evaluate_results = evaluate_article_with_gpt(articles)
        
        # å…³è”è¯„ä¼°ç»“æœ
        for evaluate in evaluate_results:
            for article in articles:
                if article.link == evaluate.get("link"):
                    article.evaluate = evaluate

        # è¿‡æ»¤ä½åˆ†æ–‡ç« 
        articles = [item for item in articles if item.evaluate and item.evaluate.get("score", 0) >= 7]
        if not articles:
            continue

        # æŒ‰è¯„åˆ†æ’åº
        articles.sort(key=lambda x: x.evaluate["score"], reverse=True)
        
        # å»é‡å¤„ç†
        filtered_articles = []
        for article in articles:
            title = article.evaluate["title"]
            # ç§»é™¤emojiå’Œç©ºæ ¼åçš„æ ‡é¢˜ç”¨äºæ¯”è¾ƒ
            clean_title = ''.join(c for c in title if not is_emoji(c)).strip()
            
            # æ£€æŸ¥æ˜¯å¦ä¸å·²æœ‰æ ‡é¢˜ç›¸ä¼¼
            is_duplicate = False
            for seen_title in seen_titles:
                if is_similar_title(clean_title, seen_title):
                    # å¦‚æœæ–°æ–‡ç« è¯„åˆ†æ›´é«˜ï¼Œæ›¿æ¢æ—§æ–‡ç« 
                    if article.evaluate["score"] > seen_titles[seen_title].evaluate["score"]:
                        filtered_articles.remove(seen_titles[seen_title])
                        filtered_articles.append(article)
                        seen_titles[clean_title] = article
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                filtered_articles.append(article)
                seen_titles[clean_title] = article

        # é€‰æ‹©æ–‡ç« 
        output_count = articles[0].config.get("output_count", 2)
        select_articles = filtered_articles[:output_count]
        show_articles.extend(select_articles)
        
    # æœ€ç»ˆæ’åºå’Œæ•°é‡é™åˆ¶
    show_articles.sort(key=lambda x: x.evaluate["score"], reverse=True)
    return show_articles[:max_article_nums]

def is_emoji(c):
    """åˆ¤æ–­å­—ç¬¦æ˜¯å¦ä¸ºemoji"""
    return c in ['ğŸ¤–', 'ğŸ’¡', 'ğŸ”¬', 'ğŸ“±', 'ğŸš€', 'âš¡', 'ğŸ§ ', 'ğŸ› ï¸', 'ğŸ“Š', 'ğŸ¥', 
                'ğŸ“', 'ğŸ­', 'ğŸŒ¾', 'ğŸ’°', 'ğŸ¤', 'ğŸ“ˆ', 'ğŸŒ', 'ğŸ”’', 'ğŸ“‹', 'ğŸ¯',
                'ğŸ“š', 'ğŸ¥', 'ğŸ“£', 'ğŸ’¸', 'ğŸ›¡ï¸', 'ğŸš¨']

def is_similar_title(title1, title2):
    """
    åˆ¤æ–­ä¸¤ä¸ªæ ‡é¢˜æ˜¯å¦ç›¸ä¼¼
    ä½¿ç”¨ç®€å•çš„ç›¸ä¼¼åº¦ç®—æ³•ï¼Œå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´
    """
    # å°†æ ‡é¢˜è½¬æ¢ä¸ºå°å†™è¿›è¡Œæ¯”è¾ƒ
    title1 = title1.lower()
    title2 = title2.lower()
    
    # å¦‚æœæ ‡é¢˜å®Œå…¨ç›¸åŒ
    if title1 == title2:
        return True
        
    # å¦‚æœä¸€ä¸ªæ ‡é¢˜åŒ…å«å¦ä¸€ä¸ªæ ‡é¢˜çš„å¤§éƒ¨åˆ†å†…å®¹
    if title1 in title2 or title2 in title1:
        return True
        
    # è®¡ç®—å•è¯é‡å åº¦
    words1 = set(title1.split())
    words2 = set(title2.split())
    common_words = words1.intersection(words2)
    
    # å¦‚æœå…±åŒå•è¯å æ¯”è¶…è¿‡60%ï¼Œè®¤ä¸ºæ˜¯ç›¸ä¼¼æ ‡é¢˜
    similarity = len(common_words) / max(len(words1), len(words2))
    return similarity > 0.4

def find_valid_file():
    """
    æŸ¥æ‰¾æœ‰æ•ˆçš„RSSç¼“å­˜æ–‡ä»¶
    
    Returns:
        tuple: (ç¼“å­˜ç›®å½•, ç¼“å­˜æ–‡ä»¶è·¯å¾„)
    """
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨ç¼“å­˜
    if os.environ.get("RSS_CACHE_ENABLE") != "true":
        return None, None

    current_directory = os.path.dirname(os.path.abspath(__file__))
    cache_folder = f"{current_directory}/draft"
    today_str = datetime.date.today().strftime('%Y-%m-%d')
    # æŸ¥æ‰¾ä»Šå¤©çš„ç¼“å­˜æ–‡ä»¶
    cache_files = glob.glob(f"{cache_folder}/*{today_str}.json")
    cache_file = cache_files[-1] if cache_files else None
    return cache_folder, cache_file

def save_article(articles, draft_folder):
    """
    ä¿å­˜æ–‡ç« åˆ°ç¼“å­˜æ–‡ä»¶
    
    Args:
        articles: æ–‡ç« åˆ—è¡¨
        draft_folder: ç¼“å­˜ç›®å½•
    """
    data = []
    path = f"{draft_folder}/article_cache_{datetime.date.today().strftime('%Y-%m-%d')}.json"
    # å°†æ–‡ç« å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸
    for article in articles:
        data.append(article.__dict__)

    # ä¿å­˜ä¸ºJSONæ–‡ä»¶
    with open(path, "w") as fp:
        fp.write(json.dumps(data, indent=4))

def decode_article(path):
    """
    ä»ç¼“å­˜æ–‡ä»¶è§£ææ–‡ç« 
    
    Args:
        path: ç¼“å­˜æ–‡ä»¶è·¯å¾„
    
    Returns:
        list: æ–‡ç« å¯¹è±¡åˆ—è¡¨
    """
    rss_list = []
    with open(path, "r") as fp:
        object_list = json.loads(fp.read())
        # å°†JSONæ•°æ®è½¬æ¢å›æ–‡ç« å¯¹è±¡
        for item in object_list:
            rss_item = rss.Article()
            for key, value in item.items():
                setattr(rss_item, key, value)
            rss_list.append(rss_item)
    return rss_list
